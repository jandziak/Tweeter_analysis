{
    "collab_server" : "",
    "contents" : "library(RTextTools)\nlibrary(e1071)\n\npos_tweets =  rbind(\n  c('I love this car', 'positive'),\n  c('This view is amazing', 'positive'),\n  c('I feel great this morning', 'positive'),\n  c('I am so excited about the concert', 'positive'),\n  c('He is my best friend', 'positive')\n)\n\nneg_tweets = rbind(\n  c('I do not like this car', 'negative'),\n  c('This view is horrible', 'negative'),\n  c('I feel tired this morning', 'negative'),\n  c('I am not looking forward to the concert', 'negative'),\n  c('He is my enemy', 'negative')\n)\n\ntest_tweets = rbind(\n  c('feel happy this morning', 'positive'),\n  c('larry friend', 'positive'),\n  c('not like that man', 'negative'),\n  c('house not great', 'negative'),\n  c('your song annoying', 'negative')\n)\n\ntweets = rbind(pos_tweets, neg_tweets, test_tweets)\n  # build dtm\nmatrix= create_matrix(tweets[,1], language=\"english\", \n                      removeStopwords=FALSE, removeNumbers=TRUE, \n                      stemWords=FALSE) \n\n# train the model\nmat = as.matrix(matrix)\nclassifier = naiveBayes(mat[1:10,], as.factor(tweets[1:10,2]))\n\n# test the validity\npredicted = predict(classifier, mat[11:15,])\ntable(tweets[11:15, 2], predicted)\nrecall_accuracy(tweets[11:15, 2], predicted)\n\n# build the data to specify response variable, training set, testing set.\ncontainer = create_container(matrix, as.numeric(as.factor(tweets[,2])),\n                             trainSize=1:10, testSize=11:15,virgin=FALSE)\n\nmodels = train_models(container, algorithms=c(\"MAXENT\" ,\n                                              \"SVM\", \n                                              \"RF\", \n                                              \"BAGGING\", \n                                              \"TREE\"))\n\nresults = classify_models(container, models)\n\n# accuracy table\ntable(as.numeric(as.factor(tweets[11:15, 2])), results[,\"FORESTS_LABEL\"])\ntable(as.numeric(as.factor(tweets[11:15, 2])), results[,\"MAXENTROPY_LABEL\"])\n\n# recall accuracy\nrecall_accuracy(as.numeric(as.factor(tweets[11:15, 2])), results[,\"FORESTS_LABEL\"])\nrecall_accuracy(as.numeric(as.factor(tweets[11:15, 2])), results[,\"MAXENTROPY_LABEL\"])\nrecall_accuracy(as.numeric(as.factor(tweets[11:15, 2])), results[,\"TREE_LABEL\"])\nrecall_accuracy(as.numeric(as.factor(tweets[11:15, 2])), results[,\"BAGGING_LABEL\"])\nrecall_accuracy(as.numeric(as.factor(tweets[11:15, 2])), results[,\"SVM_LABEL\"])\n  \n# model summary\nanalytics = create_analytics(container, results)\nsummary(analytics)\nhead(analytics@document_summary)\nanalytics@ensemble_summar\n\nN=4\nset.seed(2014)\ncross_validate(container,N,\"MAXENT\")\ncross_validate(container,N,\"TREE\")\ncross_validate(container,N,\"SVM\")\ncross_validate(container,N,\"RF\")\nThe results can be found on my Rpub page. It seems that maxent reached the same \nrecall accuracy as naive Bayes. The other methods even did a worse job.\nThis is understandable, since we have only a very small data set. \nTo enlarge the training set, we can get a much better results for \nsentiment analysis of tweets using more sophisticated methods. \nI will show the results with anther example.\n\nSentiment analysis for tweets\nThe data comes from victorneo. victorneo shows how to do sentiment analysis \nfor tweets using Python. Here, I will demonstrate how to do it in R.\n\nRead data:\n  \n  ###################\n  \"load data\"\n###################\nsetwd(\"D:/Twitter-Sentimental-Analysis-master/\")\nhappy = readLines(\"./happy.txt\")\nsad = readLines(\"./sad.txt\")\nhappy_test = readLines(\"./happy_test.txt\")\nsad_test = readLines(\"./sad_test.txt\")\n\ntweet = c(happy, sad)\ntweet_test= c(happy_test, sad_test)\ntweet_all = c(tweet, tweet_test)\nsentiment = c(rep(\"happy\", length(happy) ), \n              rep(\"sad\", length(sad)))\nsentiment_test = c(rep(\"happy\", length(happy_test) ), \n                   rep(\"sad\", length(sad_test)))\nsentiment_all = as.factor(c(sentiment, sentiment_test))\n\nlibrary(RTextTools)\n# naive bayes\nmat= create_matrix(tweet_all, language=\"english\", \n                   removeStopwords=FALSE, removeNumbers=TRUE, \n                   stemWords=FALSE, tm::weightTfIdf)\n\nmat = as.matrix(mat)\n\nclassifier = naiveBayes(mat[1:160,], as.factor(sentiment_all[1:160]))\npredicted = predict(classifier, mat[161:180,]); predicted\n\ntable(sentiment_test, predicted)\nrecall_accuracy(sentiment_test, predicted)\nThen, try the other methods:\n  \n  # the other methods\nmat= create_matrix(tweet_all, language=\"english\", \n                   removeStopwords=FALSE, removeNumbers=TRUE, \n                   stemWords=FALSE, tm::weightTfIdf)\n\ncontainer = create_container(mat, as.numeric(sentiment_all),\n                             trainSize=1:160, testSize=161:180,virgin=FALSE) #可以设置removeSparseTerms\n\nmodels = train_models(container, algorithms=c(\"MAXENT\",\n                                              \"SVM\",\n                                              #\"GLMNET\", \"BOOSTING\", \n                                              \"SLDA\",\"BAGGING\", \n                                              \"RF\", # \"NNET\", \n                                              \"TREE\" \n))\n\n# test the model\nresults = classify_models(container, models)\ntable(as.numeric(as.numeric(sentiment_all[161:180])), results[,\"FORESTS_LABEL\"])\nrecall_accuracy(as.numeric(as.numeric(sentiment_all[161:180])), results[,\"FORESTS_LABEL\"])\nHere we also want to get the formal test results, including:\n  \nanalytics@algorithm_summary: Summary of precision, recall, f-scores, and accuracy sorted by topic code for each algorithm\nanalytics@label_summary: Summary of label (e.g. Topic) accuracy\nanalytics@document_summary: Raw summary of all data and scoring\nanalytics@ensemble_summary: Summary of ensemble precision/coverage. Uses the n variable passed into create_analytics()\nNow let’s see the results:\n  \n  # formal tests\nanalytics = create_analytics(container, results)\nsummary(analytics)\n\nhead(analytics@algorithm_summary)\nhead(analytics@label_summary)\nhead(analytics@document_summary)\nanalytics@ensemble_summary # Ensemble Agreement\n\n# Cross Validation\nN=3\ncross_SVM = cross_validate(container,N,\"SVM\")\ncross_GLMNET = cross_validate(container,N,\"GLMNET\")\ncross_MAXENT = cross_validate(container,N,\"MAXENT\")\n\n",
    "created" : 1467024800206.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2291597912",
    "id" : "371929C1",
    "lastKnownWriteTime" : 1467026813,
    "last_content_update" : -2147483648,
    "path" : "C:/Repository/Tweeter/tweet_models.R",
    "project_path" : "tweet_models.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}